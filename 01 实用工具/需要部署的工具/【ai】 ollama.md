
[Ollama](https://ollama.com/)

![](附件/Pasted%20image%2020250122151604.png)

**Ollama** 是一个开源项目，旨在帮助用户轻松运行和管理本地的大型语言模型（LLMs）。它特别适合那些希望在本地环境中使用 AI 模型，同时注重隐私和离线能力的用户。以下是 Ollama 的主要特点和功能：


## 主要特点：
1. **本地运行**：
   - 支持在本地设备上运行开源的大型语言模型，无需依赖云端服务。
   - 数据完全在本地处理，确保隐私和安全。

2. **模型管理**：
   - 提供简单的命令行工具，方便用户下载、更新和管理不同的语言模型。
   - 支持多种开源模型，如 LLaMA、GPT-J 等。

3. **轻量级**：
   - 设计简洁，资源占用低，适合在个人电脑或服务器上运行。

4. **跨平台支持**：
   - 支持多种操作系统，包括 Linux、macOS 和 Windows。

## 使用场景：
1. **隐私保护**：
   - 适合处理敏感数据或需要高度隐私保护的任务。
2. **离线使用**：
   - 在没有网络连接的环境中，仍能使用 AI 模型进行文本生成或分析。
3. **开发与测试**：
   - 开发者可以在本地环境中测试和调试 AI 模型，无需依赖外部 API。

## 安装与使用：
1. **安装**：
   - 通过命令行工具安装 Ollama，具体步骤可参考其[官方文档](https://ollama.ai)。
2. **下载模型**：
   - 使用命令 `ollama pull <模型名称>` 下载所需的语言模型。
3. **运行模型**：
   - 使用命令 `ollama run <模型名称>` 启动模型并进行交互。

## 修改模型存放路径
1. **设置环境变量**：
   - 打开系统设置，进入【高级系统设置】→【环境变量】。
   - 在【系统变量】中新建变量：
     - 变量名：`OLLAMA_MODELS`
     - 变量值：自定义路径（如 `D:\AI\Ollama`）。
   - 保存后重启系统或通过 `cmd` 执行 `set` 命令检查是否生效。

2. **迁移已有模型**：
   - 将原路径（默认在用户目录下的 `.ollama` 文件夹）中的模型文件移动到新路径。
   - 使用 `ollama list` 命令检查迁移是否成功。

3. **验证**：
   - 执行 `ollama pull` 或 `ollama run` 命令，确认模型下载或运行路径已更新。



<span style="background:#b1ffff">以下是一个使用 `deepseek-r1` 模型的示例，展示如何通过命令行运行模型并进行交互：</span>

## 示例：使用 `deepseek-r1:7b` 模型

1. **安装 Ollama**（如果尚未安装）：
   - 前往 [Ollama 下载页面](https://ollama.com/download) 下载并安装 Ollama。

2. **拉取模型**：
   - 打开终端或命令行工具，运行以下命令来拉取 `deepseek-r1:7b` 模型：
     ```bash
     ollama pull deepseek-r1:7b
     ```

3. **运行模型**：
   - 拉取完成后，运行以下命令启动模型并进行交互：
     ```bash
     ollama run deepseek-r1:7b
     ```

4. **与模型交互**：
   - 启动后，你可以直接输入问题或指令，模型会生成响应。例如：
     ```
     > 请解释一下量子计算的基本原理。
     ```
   - 模型会生成相应的回答，你可以继续提问或结束会话。

### 示例输出
```
> 请解释一下量子计算的基本原理。

量子计算是一种基于量子力学原理的计算方式，利用量子比特（qubit）进行信息处理。与经典计算机使用的二进制比特（0 或 1）不同，量子比特可以同时处于多个状态的叠加态。这种特性使得量子计算机在某些问题上具有指数级的计算优势，例如大数分解和量子模拟。
```

### 其他模型
你可以根据需要选择不同的模型版本，例如 `deepseek-r1:1.5b`、`deepseek-r1:14b` 等，只需将命令中的模型名称替换为所需的版本即可。

